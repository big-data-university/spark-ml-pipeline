{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// 导入需要的包\n",
    "import org.apache.spark.ml.{Model, Pipeline}\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{Tokenizer, Word2Vec}\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.sql.{DataFrame, SQLContext}\n",
    "\n",
    "import scala.io.Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from s3 or aliyun\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// 常量定义\n",
    "final val LABEL_POSITIVE = 1.0\n",
    "final val LABEL_NEGATIVE = 0.0\n",
    "\n",
    "final val CLASS_SPAM = \"spam\"\n",
    "\n",
    "final val TRAIN_DATA = \"https://s3.amazonaws.com/workflowexecutor/examples/data/SMSSpamCollection.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// 辅助方法定义\n",
    "\n",
    "/**\n",
    " * 数据格式转换\n",
    " */\n",
    "def loadData(filePath: String): (Vector[(String, Double)], Vector[(String, Double)]) = {\n",
    "    def reformat(sms: String, label: Double): (String, Double) = (sms.split(\"\\t\").last, label)\n",
    "\n",
    "    val file = Source.fromURL(filePath,\"UTF-8\").getLines().toVector.tail\n",
    "    val (spam, ham) = file.partition(_.contains(CLASS_SPAM))\n",
    "    val spamData = spam.map(x => reformat(x, LABEL_POSITIVE))\n",
    "    val hamData = ham.map(x => reformat(x, LABEL_NEGATIVE))\n",
    "    (spamData, hamData)\n",
    "}\n",
    "\n",
    "/**\n",
    " * 计算精确度\n",
    " */\n",
    "def precision(model:Model[_], test:DataFrame):Double = {\n",
    "    val testResult = model.transform(test)\n",
    "    val total = testResult.count()\n",
    "    val corrects = testResult.filter(\"prediction = label\").count()\n",
    "\n",
    "    corrects.asInstanceOf[Double] / total\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// 数据集加载\n",
    "val sqlContext = new SQLContext(sc)\n",
    "\n",
    "import sqlContext.implicits._\n",
    "\n",
    "val (spam, ham) = loadData(TRAIN_DATA)\n",
    "\n",
    "// 随机选取训练数据和测试数据\n",
    "val (trainSpan, testSpan) = spam.partition(_._1.length % 10 < 8)\n",
    "val (trainHam, testHam) = ham.partition(_._1.length % 10 < 8)\n",
    "\n",
    "val negTest = sc.parallelize(testSpan).toDF(\"text\", \"label\").cache()\n",
    "val posTest = sc.parallelize(testHam).toDF(\"text\", \"label\").cache()\n",
    "\n",
    "val train = sc.parallelize(trainHam.union(trainSpan)).toDF(\"text\", \"label\").cache()\n",
    "val test = negTest.unionAll(posTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// 构造流水线\n",
    "val tokenizer = new Tokenizer().setInputCol(\"text\").setOutputCol(\"words\")\n",
    "\n",
    "val word2vec = new Word2Vec().setInputCol(tokenizer.getOutputCol).setOutputCol(\"features\")\n",
    "\n",
    "val lr = new LogisticRegression()\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(tokenizer, word2vec, lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### CrossValidator\n",
    "    \n",
    "    We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "    This will allow us to jointly choose parameters for all Pipeline stages.\n",
    "    A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "    We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "    With 3 values for word2vec.vectorSize and 3 values for lr.regParam,\n",
    "    this grid will have 3 x 3 = 9 parameter settings for CrossValidator to choose from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// 构造参数网络\n",
    "val paramGrid = new ParamGridBuilder().addGrid(word2vec.vectorSize, Array(50, 100, 200)).addGrid(lr.regParam, Array(0.00001, 0.001, 0.1)).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// 训练模型（包括Word2Vec特征抽取和LR分类模型）\n",
    "val cv = new CrossValidator().setEstimator(pipeline).setEvaluator(new BinaryClassificationEvaluator()).setEstimatorParamMaps(paramGrid).setNumFolds(5)\n",
    "\n",
    "val model = cv.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the best parameters extract from cross validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "val params = model.getEstimatorParamMaps\n",
    "      .zip(model.avgMetrics)\n",
    "      .maxBy(_._2)\n",
    "      ._1\n",
    "println(\"最佳参数组合\")\n",
    "println(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// 评估模型结果：使用AUC指标\n",
    "val testResult = model.transform(test)\n",
    "val evaluator = new BinaryClassificationEvaluator()\n",
    "println(s\"test metrics: ${evaluator.evaluate(testResult)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// 评估模型结果：负样本与正样本准确率\n",
    "val negPrecision = precision(model,negTest)\n",
    "val posPrecision = precision(model,posTest)\n",
    "println(s\"neg-precision = $negPrecision, pos-precision = $posPrecision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10.4",
   "language": "scala",
   "name": "spark"
  },
  "language_info": {
   "name": "scala"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
